{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent semantic Indexing\n",
    "\n",
    "### M_s = K_s * S_s * D_s.T\n",
    "\n",
    "- M = m terms x n documents \n",
    "- M_s: only the S largest singular values \n",
    "- K_s: m terms x s importances\n",
    "- S_s = s * s sorted singular values\n",
    "- D_s.T = s * n documents\n",
    "\n",
    "#### How to answer queries:\n",
    "\n",
    "* Use cosine similarity to compaire documents: (Ds.T)i (ith column), (Ds.T)j (jth column)\n",
    "* Query = additional document\n",
    "* Mapping M -> D:  D = M.T * K * S^-1\n",
    "* **Mapping query** : q* = q.T * K.s * S.s^-1\n",
    "* **Comparing similarities**: sim(q*, d_i) = (q* dot (Ds.T)i) / (norm(q*) * norm(Ds.T_i))\n",
    "\n",
    "#### Dropping a document d3 in document space...\n",
    "The document ordering does not change even if d3 is dropped. Recall that all the documents in the term-document matrix can be considered as vectors in a  Rm dimensional vector space. Thus, since d3 has a similar magnitude and direction as d4 and d2, dropping d3 does not alter substantially the term space ( K ) and the document space ( D ) of the SVD.\n",
    "\n",
    "To modify the term and document space we should change d3 such that it in a different direction as compared to the other vectors. For example, d3 = (0, 0, 1, 1, 2, 1, 0, 0, 2, 0, 2) changes the document ordering to d2 >d4 >d1 >d3.\n",
    "\n",
    "##### Algebraic Interpretation:  \n",
    "\n",
    "recall that the matrix M transforms a unit ball into an ellipsoid, and in LSI we keep only the directions with the strongest distortion. Intuitively, if we combine linearly $d_2$ and $d_4$ with a 0.5 coefficient, we’ll find a vector that is not very dissimilar from $d_3$ (i.e., the norm is almost the same, and the direction overlaps on many components). Therefore, it’s not surprising that (in this specific example) removing $d_3$ did not lead to a different ranking. Bear in mind that, with slightly different numbers, this might not be the case anymore.\n",
    "In a real-world scenario with LSI (i.e., millions of documents) removing just a few documents rarely changes the ranking dramatically, because the documents we still keep into account will have high probability to contain the same concepts that are contained in the removed ones. That is to say, the resulting ellipsoid won’t change substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yawen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = [[1,1,1,1], \n",
    "     [0,1,1,1],\n",
    "     [1,0,0,0],\n",
    "     [0,1,0,0],\n",
    "     [1,0,0,0],\n",
    "     [1,0,1,2],\n",
    "     [1,1,1,1],\n",
    "     [1,1,1,0],\n",
    "     [1,0,0,0],\n",
    "     [0,2,1,1],\n",
    "     [0,1,1,0]]\n",
    "\n",
    "def SVD(M):\n",
    "    # compute SVD\n",
    "    K, S, Dt = np.linalg.svd(M, full_matrices=False) # K = min(M, N) if full_matrices = False\n",
    "    return K, S, Dt\n",
    "\n",
    "K, S, Dt =  SVD(M)\n",
    "\n",
    "def SVD_k(M, k):\n",
    "    K, S, Dt = SVD(M)\n",
    "    # LSI select dimensions\n",
    "    K_sel = K[:,0:k]\n",
    "    S_sel = np.diag(S)[0:k,0:k]\n",
    "    Dt_sel = Dt[0:k,:]\n",
    "    return K_sel, S_sel, Dt_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = quest; M = Term-doc matrix; k = top k documents;\n",
    "def q_star(q, M, k):\n",
    "    K, S, Dt = SVD(M)\n",
    "    \n",
    "    K_sel = K_sel = K[:,0:k]\n",
    "    S_sel = np.diag(S)[0:k,0:k]\n",
    "    #Map the query q onto the document space D as q* = qT · (K_sel · S_sel−1)\n",
    "    mapper = np.dot(K_sel, np.linalg.inv(S_sel))\n",
    "    q_trans =  np.dot( q, mapper)\n",
    "    return q_trans\n",
    "\n",
    "q = np.array([0,0,0,0,0,1,0,0,0,1,1])\n",
    "\n",
    "q_ = q_star(q, M, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.9524776244205609),\n",
       " (1, 0.9388827727147444),\n",
       " (3, 0.5931086268074786),\n",
       " (0, -0.012057913278690475)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def rank_documents(q_star, Dt, k):\n",
    "    # dict of (doc_id, cos_sim)\n",
    "    document_ranking = dict()\n",
    "    # Shrink the documents according to highest k singular values\n",
    "    Dt_sel = Dt[0:k,:]\n",
    "\n",
    "    for i in range(0, Dt_sel.shape[1]):\n",
    "        d = Dt_sel[:, i]\n",
    "        cos_sim = cosine_similarity(q_star, d)\n",
    "        document_ranking[i] = cos_sim\n",
    "    \n",
    "    # Sort according to values\n",
    "    document_ranking_sorted = sorted(document_ranking.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    return document_ranking_sorted\n",
    "\n",
    "rank_documents(q_, Dt, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['art',\n",
       " 'bake',\n",
       " 'best',\n",
       " 'book',\n",
       " 'bread',\n",
       " 'cake',\n",
       " 'comput',\n",
       " 'french',\n",
       " 'london',\n",
       " 'numer',\n",
       " 'pastri',\n",
       " 'pie',\n",
       " 'quantiti',\n",
       " 'recip',\n",
       " 'scientif',\n",
       " 'smith',\n",
       " 'without']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"bread.txt\") as f:\n",
    "# with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9980518678772611),\n",
       " (3, 0.7231078789682566),\n",
       " (2, -0.0023288750529669527),\n",
       " (4, -0.6551062911362043),\n",
       " (1, -0.6577609566355738)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Find top documents for quest 'baking', with k = 3 ####\n",
    "\n",
    "## take transpose of document vectors to convert to term document matrix.\n",
    "M = np.matrix.transpose(np.array(document_vectors))\n",
    "\n",
    "## Run LSI.\n",
    "K, S, Dt = SVD(M)\n",
    "K_sel, S_sel, Dt_sel = SVD_k(M, 3)\n",
    "\n",
    "## Prepare query\n",
    "# transform query and documents\n",
    "q = np.array([0]*len(vocabulary))\n",
    "#Set the term corresponding to baking = 1 (see vocabulary)\n",
    "q[1] = 1\n",
    "q__ = q_star(q, M, 3)\n",
    "\n",
    "# Compute similarities\n",
    "rank_documents(q__, Dt, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00416487,  0.11537136, -0.14603541])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
