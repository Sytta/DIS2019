{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Query Expansion and Indexing\n",
    "\n",
    "The following code is modified from Exercise 1. It is used to construct the vocabulary and vectorize the documents and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yawen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "#with open(\"bread.txt\") as f:\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = [tokenize(d).split() for d in original_documents]\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = set([item for sublist in documents for item in sublist])\n",
    "vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "vocabulary.sort()\n",
    "\n",
    "# compute IDF, storing idf values in a dictionary\n",
    "def idf_values(vocabulary, documents):\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        idf[term] = math.log(num_documents/sum(term in document for document in documents), math.e)\n",
    "    return idf\n",
    "\n",
    "# Function to generate the vector for a document (with normalisation)\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        vector[i] = idf[term] * counts[term]/max_count\n",
    "    return vector\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return result\n",
    "\n",
    "def vectorize_query(query, vocabulary, idf):\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    return query_vector\n",
    "    \n",
    "def search_vec(query, k):\n",
    "    query_vector = vectorize_query(query, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    ans = []\n",
    "    indices = []\n",
    "    for i in range(min(k,len(original_documents))):\n",
    "        ans.append(original_documents[scores[i][1]])\n",
    "        indices.append(scores[i][1])\n",
    "    return ans, indices, query_vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Relevance Feedback\n",
    "\n",
    "In this exercise we will implement and test Rocchio's method for user relevance feedback.\n",
    "\n",
    "Let the set of relevant documents to be $D_r $ and the set of non-relevant documents to be $D_{nr}$. Then the modified query  $\\vec{q_m}$  according to the Rocchio method is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{q_m} = \\alpha \\vec{q_0} + \\frac{\\beta}{|D_r|} \\sum_{\\vec{d_j} \\in D_r} \\vec{d_j} - \\frac{\\gamma}{|D_{nr}|} \\sum_{\\vec{d_j} \\in D_{nr}} \\vec{d_j}\n",
    "\\end{equation}\n",
    "In the Rocchio algorithm negative term weights are ignored. This means, for the negative term weights in $\\vec{q_m}$, we set them to be 0.\n",
    "\n",
    "First, complete the implementation of the Rocchio relevance feedback method, by adding the missing code for the function $expand\\_query$.   \n",
    "\n",
    "Then, compare the result obtained with the new query with the unmodified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "1 New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "2 An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "3 New at @epfl_en Life Sciences @epflSV: \"From PhD directly to Independent Group Leader\" #ELFIR_EPFL:  Early Independence Research Scholars. See https://t.co/evqyqD7FFl, also for computational biology #compbio https://t.co/e3pDCg6NVb Deadline April 1 2018 at https://t.co/mJqcrfIqkb\n",
      "4 Video of Nicola Marzari from @EPFL_en  on Computational Discovery in the 21st Century during #PASC17 now online: https://t.co/tfCkEvYKtq https://t.co/httPdHcK9W\n"
     ]
    }
   ],
   "source": [
    "ans, result_doc_ids, query_vector = search_vec(\"computer science\", 5)\n",
    "for i in range(len(ans)):\n",
    "    print(i,ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, alpha, beta, gamma):\n",
    "    \n",
    "    num_rel = len(relevant_doc_vecs)\n",
    "    num_non_rel = len(non_relevant_doc_vecs)\n",
    "    \n",
    "    # Compute the first term in the Rocchio equation\n",
    "    norm_query_vector = alpha * query_vector\n",
    "    \n",
    "    # Compute the second term in the Rocchio equation\n",
    "    norm_sum_relevant = (beta / num_rel) * np.sum(relevant_doc_vecs, axis=0) \n",
    "    \n",
    "    # Compute the last term in the Rocchio equation\n",
    "    norm_sum_non_relevant = (gamma / num_non_rel) * np.sum(non_relevant_doc_vecs, axis=0)\n",
    "    \n",
    "    # Sum all the terms\n",
    "    modified_query_vector = norm_query_vector + norm_sum_relevant - norm_sum_non_relevant\n",
    "    \n",
    "    # Ignore negative elements (so that it doesn't)\n",
    "    modified_query_vector = [x if x > 0 else 0 for x in modified_query_vector]\n",
    "    return modified_query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified query:  20162017 amp barth biophys combin comput congrat control distanc epfl eth excit httpstcoarslxzoshq httpstcob9tglxo4kd httpstcoijwbaebocj httpstcoznjk3bz6mo interview model new news no1 no8 patrick professor protein rank scienc show subject univers vdtech via world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ',\n",
       " 'New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD',\n",
       " 'An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj',\n",
       " 'International ranking: #ETH and #EPFL are top institutes',\n",
       " 'Interview (in French) de Patrick Aebischer, un \"innovation slasher\" @EPFL_en https://t.co/BtzhxEAEmN',\n",
       " 'The proteins that domesticated our genomes https://t.co/npGbUKJhBl  via @EPFL_en #VDtech https://t.co/It0SBqlKQc',\n",
       " \"New software can model natural light from the occupants' perspective https://t.co/RbMmN3Po5v via @EPFL_en #VDtech https://t.co/50enZtwUHi\",\n",
       " \"New software can model natural light from the occupants' perspective https://t.co/RbMmN3Po5v via @EPFL_en #VDtech https://t.co/lLIAvntc9R\",\n",
       " 'Latest work in our lab shows how feedback enhances brainwave control of a novel hand-exoskeleton https://t.co/VVPdX19fIM #epfl',\n",
       " '“Artificial intelligence has the potential to revolutionize transportation\" interview of Alexandre Alahi, new professor of Transportation Engineering @EPFL https://t.co/S8ZKRIAHbq RT @Trace_EPFL https://t.co/PQqkRY5ny9']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of indices marked as relevant\n",
    "# suppose first three documents were relevant and the rest were irrelevant.\n",
    "relevant_indices = [0,1,2]\n",
    "non_relevant_indices = [i for i in range(3, len(ans))]\n",
    "\n",
    "relevant_doc_ids = [result_doc_ids[i] for i in relevant_indices]\n",
    "non_relevant_doc_ids = [result_doc_ids[i] for i in non_relevant_indices]\n",
    "\n",
    "relevant_doc_vecs = [document_vectors[i] for i in relevant_doc_ids]\n",
    "non_relevant_docvecs = [document_vectors[i] for i in non_relevant_doc_ids]\n",
    "\n",
    "expanded_query = expand_query(relevant_doc_vecs, non_relevant_doc_vecs, query_vector, 1, 1, 1)\n",
    "\n",
    "new_query = ' '.join([vocabulary[i] for i, val in enumerate(expanded_query) if val>0])\n",
    "\n",
    "new_ans , not_important_now, idontcare_anymore = search_vec(new_query, 10)\n",
    "\n",
    "print('Modified query: ', new_query)\n",
    "new_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6510, 6510)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_relevant_doc_vecs[0]), len(relevant_doc_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Link based ranking\n",
    "\n",
    "### Preliminaries\n",
    "If you want to normalize a vector to L1-norm or L2-norm, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1-norm of [1 2 3] is [0.16666667 0.33333333 0.5       ]\n",
      "L2-norm of [1 2 3] is [0.26726124 0.53452248 0.80178373]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "pr = np.array([1,2,3])\n",
    "print(\"L1-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,1)))\n",
    "print(\"L2-norm of {0} is {1}\".format(pr, pr / np.linalg.norm(pr,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Page Rank (Eigen-vector method)\n",
    "Consider a tiny Web with three pages A, B and C with no inlinks,\n",
    "and with initial PageRank = 1. Initially, none of the pages link to\n",
    "any other pages and none link to them. \n",
    "Answer the following questions, and calculate the PageRank for\n",
    "each question.\n",
    "\n",
    "1. Link page A to page B.\n",
    "2. Link all pages to each other.\n",
    "3. Link page A to both B and C, and link pages B and C to A.\n",
    "4. Use the previous links and add a link from page C to page B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:\n",
    "We are using the theoretical PageRank computation (without source of rank). See slide \"Transition Matrix for Random Walker\" in the lecture note. Columns of link matrix are from-vertex, rows of link matrix are to-vertex. We take the eigenvector with the largest eigenvalue.\n",
    "We only care about final ranking of the probability vector. You can choose the normalization (or not) of your choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your code here\n",
    "def pagerank_eigen(L):\n",
    "#   Construct transition probability matrix from L\n",
    "    # R is the normalised matrix of L\n",
    "    col_sums = L.sum(axis=0)\n",
    "    # To avoid division by 0\n",
    "    col_sums[col_sums == 0] = 1\n",
    "    R = L / col_sums\n",
    "    print(col_sums)\n",
    "    print(R)\n",
    "    \n",
    "#     Compute eigen-vectors and eigen-values of R\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(R)\n",
    "#     Take the eigen-vector with maximum eigven-value\n",
    "    p = eigenvectors[:,np.argmax(np.absolute(eigenvalues))]\n",
    "    return (R,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2]]\n",
      "[[0.  1.  0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.  0. ]]\n",
      "L=\n",
      "[[0 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]]\n",
      "\n",
      "R=\n",
      "[[0.  1.  0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.  0. ]]\n",
      "\n",
      "p=\n",
      "[[-0.74278135+0.j]\n",
      " [-0.55708601+0.j]\n",
      " [-0.37139068+0.j]]\n"
     ]
    }
   ],
   "source": [
    "# Test with the question, e.g.\n",
    "L = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,0,0]\n",
    "])\n",
    "R,p = pagerank_eigen(L)\n",
    "print(\"L=\\n{0}\\n\\nR=\\n{1}\\n\\np=\\n{2}\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Link page A to page B.\n",
    "L1 = np.matrix([\n",
    "    [0,0,0], \n",
    "    [1,0,0], \n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "# 2. Link all pages to each other.\n",
    "L2 = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,1,0]\n",
    "])\n",
    "\n",
    "# 3. Link page A to both B and C, and link pages B and C to A.\n",
    "L3 = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,0], \n",
    "    [1,0,0]\n",
    "])\n",
    "\n",
    "# 4. Use the previous links and add a link from page C to page B.\n",
    "L4 = np.matrix([\n",
    "    [0,1,1], \n",
    "    [1,0,1], \n",
    "    [1,0,0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link 1 :\n",
      "[[1 1 1]]\n",
      "[[0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "L=\n",
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]]\n",
      "\n",
      "R=\n",
      "[[0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "p=\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "\n",
      "Link 2 :\n",
      "[[2 2 2]]\n",
      "[[0.  0.5 0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.5 0. ]]\n",
      "L=\n",
      "[[0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 0]]\n",
      "\n",
      "R=\n",
      "[[0.  0.5 0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.5 0. ]]\n",
      "\n",
      "p=\n",
      "[[0.57735027]\n",
      " [0.57735027]\n",
      " [0.57735027]]\n",
      "\n",
      "\n",
      "\n",
      "Link 3 :\n",
      "[[2 1 1]]\n",
      "[[0.  1.  1. ]\n",
      " [0.5 0.  0. ]\n",
      " [0.5 0.  0. ]]\n",
      "L=\n",
      "[[0 1 1]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n",
      "\n",
      "R=\n",
      "[[0.  1.  1. ]\n",
      " [0.5 0.  0. ]\n",
      " [0.5 0.  0. ]]\n",
      "\n",
      "p=\n",
      "[[0.81649658]\n",
      " [0.40824829]\n",
      " [0.40824829]]\n",
      "\n",
      "\n",
      "\n",
      "Link 4 :\n",
      "[[2 1 2]]\n",
      "[[0.  1.  0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.  0. ]]\n",
      "L=\n",
      "[[0 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]]\n",
      "\n",
      "R=\n",
      "[[0.  1.  0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.  0. ]]\n",
      "\n",
      "p=\n",
      "[[-0.74278135+0.j]\n",
      " [-0.55708601+0.j]\n",
      " [-0.37139068+0.j]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, L in enumerate([L1, L2, L3, L4]):\n",
    "    print(\"Link {} :\".format(i + 1))\n",
    "    R,p = pagerank_eigen(L)\n",
    "    print(\"L=\\n{0}\\n\\nR=\\n{1}\\n\\np=\\n{2}\\n\\n\\n\".format(L,R,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Page Rank (Iterative method)\n",
    "\n",
    "The eigen-vector method has some numerical issues (when computing eigen-vector) and not scalable with large datasets.\n",
    "\n",
    "We will apply the iterative method in the slide \"Practical Computation of PageRank\" of the lecture.\n",
    "\n",
    "Dataset for practice: https://snap.stanford.edu/data/ca-GrQc.html. It is available within the same folder of this github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_iterative(L):\n",
    "    col_sums = L.sum(axis=0)\n",
    "    # To avoid division by 0\n",
    "    col_sums[col_sums == 0] = 1\n",
    "    R = L / col_sums\n",
    "    N = L.shape[0]\n",
    "    e = np.ones(shape=(N,1))\n",
    "    q = 0.9\n",
    "\n",
    "    p = e\n",
    "    delta = 1\n",
    "    epsilon = 0.001\n",
    "    i = 0\n",
    "    while delta > epsilon:\n",
    "        p_prev = p\n",
    "        p = q * R.dot(p)\n",
    "        p = p + ((1-q)/N) * e\n",
    "        delta = np.linalg.norm(p - p_prev, 1)\n",
    "        i += 1\n",
    "\n",
    "    print(\"Converged after {0} iterations. Ranking vector: p={1}\".format(i, p[:,0]))\n",
    "    return R,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5242\n",
      "[3466, 937, 5233]\n"
     ]
    }
   ],
   "source": [
    "# Construct link matrix from file\n",
    "n_nodes = 0\n",
    "nodes_idx = dict() #Since the nodeIDs are not from 0 to N we need to build an index of nodes\n",
    "nodes = [] #We also want to store nodeIDs to return the result of ranking vector\n",
    "\n",
    "# Read the nodes\n",
    "with open(\"ca-GrQc.txt\") as f:\n",
    "    for line in f:\n",
    "        if '#' not in line:\n",
    "            source = int(line.split()[0])\n",
    "            target = int(line.split()[1])\n",
    "            if source not in nodes_idx.keys():\n",
    "                nodes_idx[source] = n_nodes\n",
    "                nodes.append(source)\n",
    "                n_nodes += 1\n",
    "            if target not in nodes_idx.keys():\n",
    "                nodes_idx[target] = n_nodes\n",
    "                nodes.append(target)\n",
    "                n_nodes += 1\n",
    "print(n_nodes)\n",
    "print(nodes[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 52 iterations. Ranking vector: p=[[0.44049233]\n",
      " [0.33611637]\n",
      " [0.23174042]]\n",
      "Ranking vector: p=[[0.44049233]\n",
      " [0.33611637]\n",
      " [0.23174042]]\n"
     ]
    }
   ],
   "source": [
    "# Run PageRank\n",
    "R, p = pagerank_iterative(L)\n",
    "print(\"Ranking vector: p={0}\".format(p[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Ranking Methodology (Hard)\n",
    "\n",
    "1. Give a directed graph, as small as possible, satisfying all the properties mentioned below:\n",
    "\n",
    "    1. There exists a path from node i to node j for all nodes i,j in the directed\n",
    "graph. Recall, with this property the jump to an arbitrary node in PageRank\n",
    "is not required, so that you can set q = 1 (refer lecture slides).\n",
    "\n",
    "    2. HITS authority ranking and PageRank ranking of the graph nodes are different.\n",
    "\n",
    "2. Give intuition/methodology on how you constructed such a directed graph with\n",
    "the properties described in (a).\n",
    "\n",
    "3. Are there specific graph structures with arbitrarily large instances where PageRank\n",
    "ranking and HITS authority ranking are the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - Hub and Authority\n",
    "\n",
    "### a)\n",
    "\n",
    "Let the adjacency matrix for a graph of four vertices ($n_1$ to $n_4$) be\n",
    "as follows:\n",
    "\n",
    "$\n",
    "A =\n",
    "  \\begin{bmatrix}\n",
    "\t0 & 1 & 1 & 1  \\\\\n",
    "\t0 & 0 & 1 & 1 \\\\\n",
    "\t1 & 0 & 0 & 1 \\\\\n",
    "\t0 & 0 & 0 & 1 \\\\\n",
    "  \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Calculate the authority and hub scores for this graph using the\n",
    "HITS algorithm with k = 6, and identify the best authority and\n",
    "hub nodes.\n",
    "\n",
    "### b)\n",
    "Apply the HITS algorithm to the dataset: https://snap.stanford.edu/data/ca-GrQc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** We follow the slide \"HITS algorithm\" in the lecture. **Denote $x$ as authority vector and $y$ as hub vector**. You can use matrix multiplication for the update steps in the slide \"Convergence of HITS\". Note that rows of adjacency matrix is from-vertex and columns of adjacency matrix is to-vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran a total of 4 iterations with the convergence rate delta1, delta2=0.0006129616443491803,0.0016852780559584724\n",
      "Result using iterative method:\n",
      " Authoriy vector x=[0.16854345 0.27253834 0.49794598 0.80583234]\n",
      " Hub vector y=[0.65541849 0.54207544 0.40532459 0.33510118]\n"
     ]
    }
   ],
   "source": [
    "# A = adjacency matrix => need to be an np.array, not matrix!!\n",
    "def hits_iterative(A, k = 10):\n",
    "    N = A.shape[0]\n",
    "    a0, h0 = 1 / (N*N) * np.ones(N), 1 / (N*N) * np.ones(N) \n",
    "    aprev, hprev = a0, h0\n",
    "    delta1 = delta2 = 1\n",
    "    epsilon = 0.001 # We can strictly check for convergence rate of HITS algorithm\n",
    "    l = 0\n",
    "    while l < k and delta1 > epsilon and delta2 > epsilon:\n",
    "        h = np.matmul(A, aprev) # hub: how many authority i point to?\n",
    "        a = np.matmul(np.transpose(A), h) # authority: how many hubs points to me?\n",
    "        \n",
    "        a = a / np.linalg.norm(a,2)\n",
    "        h = h / np.linalg.norm(h,2)\n",
    "        \n",
    "        delta1 = np.linalg.norm(a-aprev,1)\n",
    "        delta2 = np.linalg.norm(h-hprev,1)\n",
    "        aprev = a\n",
    "        hprev = h\n",
    "        l += 1\n",
    "    \n",
    "    print(\"Ran a total of {0} iterations with the convergence rate delta1, delta2={1},{2}\".format(l, delta1, delta2))\n",
    "    return aprev, hprev\n",
    "\n",
    "A=np.array([\n",
    "    [0, 1, 1, 1], # row1: nb links node 1 points to\n",
    "    [0, 0, 1, 1], \n",
    "    [1, 0, 0, 1],\n",
    "    [0, 0, 0, 1],\n",
    "])  #col1 : nb links pointing to node 1 (different than PageRank's Link matrix!!)\n",
    "\n",
    "a, h = hits_iterative(A, 100)\n",
    "print(\"Result using iterative method:\\n Authoriy vector x={0}\\n Hub vector y={1}\".format(a, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
